<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>Diffusion Models for Video Generation | Lil&#39;Log</title>
    <link rel="stylesheet" href="https://llbbcc.github.io/stylesheet.css">
</head>

<body>
    <main class="main">
        <article class="post-single">
            <header class="post-header">
                <h1 class="post-title">Diffusion Models for Video Generation</h1>
                <div class="post-meta">Date: April 12, 2024 | Estimated Reading Time: 20 min | Author: Lilian Weng</div>
            </header>
            <div class="post-content">
                <p>Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder taskâ€”using it for video generation.</p>
                <h2>Introduction</h2>
                <p>The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:</p>
                <ul>
                    <li>It has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.</li>
                    <li>In comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.</li>
                </ul>
                <!-- Additional content can be included here -->
            </div>
        </article>
    </main>
</body>

</html>
